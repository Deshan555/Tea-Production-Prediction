# -*- coding: utf-8 -*-
"""tea_weather.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-XwQm0iFbJFiQAyVZn9Ag9exw5t0FaZQ
"""



from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

exp_data = pd.read_csv('/content/drive/MyDrive/Data Science/Model/weather.csv')
exp_data2 = pd.read_csv('/content/drive/MyDrive/Data Science/Model/weather2.csv')
exp_data3 = pd.read_csv('/content/drive/MyDrive/Data Science/Model/weather3.csv')

exp_data.head()

exp_data2.head()

exp_data3.head()

tea_records = exp_data[exp_data['Crop'] == 'Sugarcane']
tea_records2 = exp_data2[exp_data2['Crop'] == 'Sugarcane']
tea_records3 = exp_data3[exp_data3['Crop'] == 'Sugarcane']

sugar_cane_records = []
for data in [exp_data, exp_data2, exp_data3]:
    sugar_cane_records.append(data[data['Crop'] == 'Sugarcane'])

# Concatenate the filtered records into one dataset
combined_tea_records = pd.concat(sugar_cane_records, ignore_index=True)

combined_tea_records.head()

combined_tea_records.count()

combined_tea_records.shape

fields_to_drop = ['State_Name', 'District_Name', 'Crop']

# Drop the specified fields
combined_tea_records.drop(columns=fields_to_drop, inplace=True)

combined_tea_records.shape

combined_tea_records.head()

combined_tea_records.tail()

Y = combined_tea_records['Production']
X = combined_tea_records.drop("Production", axis = 1)

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
categorical_features = ["Season"]
one_hot = OneHotEncoder()
transformed = ColumnTransformer([("one_hot", one_hot, categorical_features)], remainder="passthrough")

transformed_x = transformed.fit_transform(X)
transformed_x

from sklearn.ensemble import RandomForestRegressor

# Model
model = RandomForestRegressor()
np.random.seed(42)
X_train, X_test, Y_train, Y_test = train_test_split(transformed_x, Y, test_size=0.2)
model.fit(X_train, Y_train)
model.score(X_test, Y_test)

transformed_x.shape

import joblib

joblib.dump(model, 'model_tea_weather.pkl')

import joblib

# Assuming 'model' is your trained model object
model_path_joblib = 'model_tea_weather.joblib'
joblib.dump(model, model_path_joblib)

import h5py

# Assuming 'model' is your trained model object
model_path_h5 = 'model_tea_weather.h5'
with h5py.File(model_path_h5, 'w') as hf:
    hf.create_dataset('model_weights', data=model.get_weights())

input_data = (2010, 1, 750, 30, 1500, 80, 90)



# change the input data to a numpy array
input_data_as_numpy_array= np.asarray(input_data)

# reshape the numpy array as we are predicting for only on instance
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

prediction = model.predict(input_data_reshaped)
print(prediction)